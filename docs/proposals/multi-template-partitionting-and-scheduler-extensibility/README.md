---
title: Multi-Template Partitioning & Scheduler Extensibility
authors:
- "@Vacant2333"
reviewers:
- TBD
approvers:
- TBD

creation-date: 2024-07-18

---

# Multi-Template Partitioning & Scheduler Extensibility

## Background

<!--
Êàë‰ª¨ÁõÆÂâçÊ≠£Âú® Karmada ÁöÑÂü∫Á°Ä‰∏äÔºåÂÆûÁé∞Â§öÈõÜÁæ§ÁéØÂ¢É‰∏ãÁöÑ AI ËÆ°ÁÆó‰ªªÂä°Ë∞ÉÂ∫¶„ÄÇ

Â§ßÈÉ®ÂàÜÊåëÊàòÂ∑≤ÁªèÂæóÂà∞Â¶•ÂñÑËß£ÂÜ≥„ÄÇ‰æãÂ¶ÇÔºå‰∏∫‰∫ÜÁÆ°ÁêÜ‰ªªÂä°ÈòüÂàóÔºåÊàë‰ª¨Âà©Áî® Mutating Webhook ÊöÇÂÅúÊñ∞Âª∫ÁöÑ ResourceBinding„ÄÇ
ÈöèÂêéÔºåÊàë‰ª¨ÁöÑËá™ÂÆö‰πâ Controller ‰ºöÊ†πÊçÆÈòüÂàóÈúÄÊ±ÇÈÄê‰∏ÄÊøÄÊ¥ªËøô‰∫õ ResourceBindingÔºåÂπ∂‰∫§Áî± Karmada Scheduler Â§ÑÁêÜ„ÄÇ
ÂêåÊó∂ÔºåÊàë‰ª¨‰πüÂú® Karmada Á§æÂå∫ÊåÅÁª≠Êé®Âä®‰ªªÂä°‰ºòÂÖàÁ∫ßÁöÑÂèëÂ±ï„ÄÇ

Êàë‰ª¨Âú®Â§öÈõÜÁæ§ÁéØÂ¢É‰∏ãË∞ÉÂ∫¶ AI ËÆ°ÁÆó‰ªªÂä°Êó∂ÈÅáÂà∞‰∫Ü‰∏Ä‰∫õÊ£òÊâãÁöÑÈóÆÈ¢ò„ÄÇËøô‰∫õ‰ªªÂä°‰∏ªË¶ÅÂàÜ‰∏∫ÂçïÊ®°ÊùøÂíåÂ§öÊ®°Êùø‰∏§ÁßçÁ±ªÂûãÔºåÂêÑÈúÄ‰∏çÂêåÁöÑË∞ÉÂ∫¶Á≠ñÁï•Êù•ÂÆûÁé∞ gang Ë∞ÉÂ∫¶„ÄÇ
‰æãÂ¶ÇÔºå‰∏Ä‰∏™Â∏∏ËßÅÁöÑÈóÆÈ¢òÊòØÊØè‰∏™‰ªªÂä°ÈÉΩËÆæÊúâÊúÄÂ∞èÂèØÁî® Pod Êï∞ÈáèÔºàMinAvailableÔºâÔºåËøô‰∏ÄÊï∞ÈáèÊ†áËØÜ‰∫Ü‰ªªÂä°ËøêË°åÊâÄÈúÄÁöÑÊúÄ‰Ωé Pod Êï∞ÔºåÂπ∂‰∏çÊÄªÊòØ‰∏é Spec.replica Áõ∏Á≠âÔºå
Âõ†Ê≠§ÂÖÅËÆ∏ÁöÑPodÊï∞ÈáèËåÉÂõ¥Â∫î‰∏∫[Spec.MinAvailable, Spec.Replica]„ÄÇÊ≠§Â§ñÔºåÂ§öÊ®°Êùø‰ªªÂä°Â¶ÇÊûúÊãÜÂàÜÂà∞‰∏çÂêåÈõÜÁæ§Â∞ÜÂèòÂæóÂ§çÊùÇÔºå
Êàë‰ª¨Â∏åÊúõËøôÁ±ª‰ªªÂä°ËÉΩÂ§üÂè™Ë¢´Ë∞ÉÂ∫¶Âà∞Âçï‰∏ÄÈõÜÁæ§ÔºåÈÅøÂÖçË∑®ÈõÜÁæ§ÊãÜÂàÜ„ÄÇ
‰∏∫Ëß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨Ê≠£Êé¢Á¥¢ÂºïÂÖ•Êñ∞ÁöÑË∞ÉÂ∫¶Á≠ñÁï•ÔºåÂπ∂Â∏åÊúõÂ¢ûÂº∫ Karmada Scheduler ÁöÑÁÅµÊ¥ªÊÄßÔºåÂÖÅËÆ∏Âú® AssignReplica Èò∂ÊÆµÊ≥®ÂÜåËá™ÂÆö‰πâÊèí‰ª∂„ÄÇ

ÂêåÊó∂ÊàëËÆ§‰∏∫ËøôÊòØÂØπ Karmada ÊúâÁõäÁöÑÔºåÂÆÉÂèØ‰ª•‰∏∞ÂØå Karmada ÊîØÊåÅÁöÑÂú∫ÊôØÔºå‰ª•ÂèäÊèêÈ´ò Karmada ÁöÑÁÅµÊ¥ªÊÄß„ÄÇÂΩìÁî®Êà∑ÊúâËá™Â∑±ÁöÑË∞ÉÂ∫¶Á≠ñÁï•Êó∂Ôºå‰ª•ÁõÆÂâçÁöÑÊÉÖÂÜµÊù•ÁúãÔºå
Áî®Êà∑ÂøÖÈ°ªÈÄöËøá‰øÆÊîπ Karmada ÁöÑÊ†∏ÂøÉ APIÔºåËøôÊòØÈùûÂ∏∏Â§çÊùÇÁöÑ‰∏ÄÁßçÊñπÂºè„ÄÇËÄåÈÄöËøáÊâ©Â±ïÁÇπÊù•ÂÆûÁé∞ÂàôËæÉ‰∏∫ÁÆÄÂçïÔºåÂÆÉÊòØ‰∏ÄÁßç Out of Tree ÁöÑÊñπÂºè„ÄÇ
-->

**We are currently building on Karmada to implement AI computing task scheduling in a multi-cluster environment.**

Most challenges have already been addressed. For instance, to manage the task queue, we utilize a Mutating Webhook to
pause newly created ResourceBindings. Subsequently, our custom Controller activates these ResourceBindings one by one
according to the queue demands and hands them over to the Karmada Scheduler. We also continue to promote the development
of [task priority](https://github.com/karmada-io/karmada/pull/4993) within the Karmada community.

**We are facing some difficult issues when scheduling AI computing tasks in a multi-cluster environment.** These tasks
are primarily divided into single-template and multi-template types, each requiring different scheduling strategies for
gang scheduling. A common issue is that each task has a minimum available Pod number (MinAvailable), which indicates the
lowest number of Pods needed to run the task and does not always equal Spec.replica. Therefore, the permissible range of
Pod numbers should be [Spec.MinAvailable, Spec.Replica]. Moreover, multi-template tasks become complex if split across
different clusters, and we prefer these tasks to be scheduled only to a single cluster to avoid cross-cluster splitting.
To address these issues, we are exploring new scheduling strategies and aiming to enhance the flexibility of
Karmada Scheduler by allowing the registration of custom plugins during the AssignReplica phase.

**I believe this is beneficial for Karmada** as it can enrich the scenarios supported by Karmada and improve its flexibility.
Currently, if users have their own scheduling strategies, they must modify Karmada's core API, which is a very complex method.
Implementing through extension points is simpler, representing an Out of Tree approach.

Relevant contents:
- https://github.com/karmada-io/karmada/issues/3318
- https://github.com/karmada-io/karmada/issues/3318#issuecomment-1682519228
- https://docs.google.com/document/d/1l6zO4xf879KdW_WPS7aMED0SUmnk487_XDsC12TtuTQ/edit?disco=AAAAv4FJT8E

## Summary

<!--
ÂΩìÂâçÁöÑ Karmada Scheduler ÊîØÊåÅÂú® FilterCluster Âíå ScoreCluster Èò∂ÊÆµÊ≥®ÂÜåÊèí‰ª∂Ôºå‰ΩÜÂú® SelectCluster Âíå
ReplicaScheduling (AssignReplicas) Èò∂ÊÆµÂàô‰∏çÊîØÊåÅ„ÄÇÊàë‰ª¨Â∏åÊúõËÉΩÂú® ReplicaScheduling (AssignReplicas) Èò∂ÊÆµÊîØÊåÅÊ≥®ÂÜåËá™ÂÆö‰πâÊèí‰ª∂Ôºå‰ª•ÊèêÂçá Karmada ÁöÑÁÅµÊ¥ªÊÄß„ÄÇ

Êàë‰ª¨Â∞ÜÊèê‰æõ‰∏ÄÁßçÊñπÊ°àÔºå‰ΩøÂæóÂú® ReplicaScheduling (AssignReplicas) Èò∂ÊÆµËÉΩÂ§üÊîØÊåÅÊ≥®ÂÜåËá™ÂÆö‰πâÊèí‰ª∂„ÄÇÂêåÊó∂ÂåÖÊã¨Êñ∞ÁöÑÂú®PropagationPolicy‰∏≠ÈÄâÊã©
Êèí‰ª∂/Á≠ñÁï•ÁöÑÊñπÂºèÔºå‰ª•ÂèäÊèí‰ª∂ÁöÑÁÆ°ÁêÜÁ≠ñÁï•„ÄÇ
-->

The current Karmada Scheduler supports plugin registration during the **FilterCluster** and **ScoreCluster** stages, but not during the **SelectCluster** and  
**ReplicaScheduling (AssignReplicas)** stages. We aim to enable custom plugin registration during the **ReplicaScheduling (AssignReplicas)** stage to enhance Karmada's flexibility.

![schedule-process](statics/schedule-process.png)

We will provide a solution that allows for the registration of custom plugins during the **ReplicaScheduling (AssignReplicas)** stage.
This includes methods for handling multiple results from multiple plugins and strategies for plugin management.

### Goals

- Support custom plugins in the `AssignReplicas` stage.
- Allow users to add custom strategies and select custom strategies in the PropagationPolicy.
- Properly manage custom plugins contributed by users to the community.
- Provide design ideas for multi-template splitting.

## Proposal

### User Story

#### Story 1: Precise Cluster Scheduling for AI Tasks

<!--
‰Ωú‰∏∫‰∏Ä‰∏™Áî®Êà∑ÔºåÊàëÂ∏åÊúõËÉΩÂ§üÂ∞Ü Karmada ‰Ωú‰∏∫‰∏Ä‰∏™ AI ‰ªªÂä°ÁöÑÁÆ°ÁêÜÂπ≥Âè∞ÔºåÂÆÉÈúÄË¶ÅÂ∞Ü**Âçï‰∏™ AI Â§ßÊï∞ÊçÆ‰ªªÂä°**‰∏ãÂèëÂà∞**Êüê‰∏Ä‰∏™ Worker Cluster**Ôºå
ÂêåÊó∂ÂÆÉÂ∫îÂΩìËÉΩÂ§ü**ÊîØÊåÅ gang Ë∞ÉÂ∫¶**ÔºåÈÅøÂÖçÈõÜÁæ§ËµÑÊ∫ê‰∏çË∂≥Êó∂‰ªçÁÑ∂‰∏ãÂèë‰ªªÂä°ÂØºËá¥ËµÑÊ∫êÊµ™Ë¥π„ÄÇ

Âú®ËøêË°å AI Â§ßÊï∞ÊçÆ‰ªªÂä°Êó∂ÔºåÁî±‰∫éÂú∞Âüü„ÄÅËÆæÂ§á„ÄÅÈõÜÁæ§Â§ßÂ∞èÁ≠âÈôêÂà∂ÔºåÊàë‰ª¨Â∏∏Â∏∏ÈúÄË¶Å‰∏∫**ÊØè‰∏™‰ªªÂä°ÈÄâÊã©ÂêàÈÄÇÁöÑ Worker Cluster** Êù•ÂÆåÊàêÂÆÉ„ÄÇ
Âú®Â§öÈõÜÁæ§‰∏äÔºå**Karmada Êó†ÁñëÊòØÊúÄÂ•ΩÁöÑÈÄâÊã©**Ôºå‰ΩÜÊòØÁõÆÂâç Scheduler ÊãìÂ±ïÊÄßÁöÑ‰∏çË∂≥ÂØºËá¥Êàë‰ª¨Êó†Ê≥ïÂÅöÂà∞ gang Ë∞ÉÂ∫¶Êù•Á≤æÁ°ÆÁöÑÈÄâÊã©‰Ωú‰∏öÈõÜÁæ§„ÄÇ
ÁâπÂà´ÊòØÂú® AI ‰Ωú‰∏ö‰∏≠Ôºå‰æãÂ¶ÇÈúÄË¶ÅÂ§ßÈáèÂπ∂Ë°åÂ§ÑÁêÜÂíåÊï∞ÊçÆÂØÜÈõÜÂûãÁöÑ‰ªªÂä°ÔºåÂΩìÂâçÁöÑË∞ÉÂ∫¶Á≠ñÁï•Ê≤°ÊúâË∂≥Â§üËÄÉËôëÂà∞ AI ‰Ωú‰∏öÂØπÁ°¨‰ª∂ÔºàÂ¶ÇGPUÔºâÁöÑÁâπÂÆöÈúÄÊ±ÇÔºå
‰ª•ÂèäËøô‰∫õ‰Ωú‰∏öÂØπÂç≥Êó∂ËµÑÊ∫êÈÖçÁΩÆÁöÑÊïèÊÑüÊÄß„ÄÇÂ¶ÇÊûú Karmada ËÉΩÂ§üÂØπ‰∏äËø∞ËÉΩÂäõËøõË°åÂÆåÂñÑÔºåËÉΩÂ§üÊã•ÊúâÂ§öÈõÜÁæ§ÊÉÖÂÜµ‰∏ãÊõ¥Â§öÂú∫ÊôØÊâÄÈúÄÁöÑËÉΩÂäõ„ÄÇ
-->

As a user, I hope to use Karmada as a management platform for AI tasks.
It needs to deploy **single AI big data tasks** to **a specific Worker Cluster**,
while also **supporting gang scheduling**
to avoid wasting resources by issuing tasks when there's insufficient cluster capacity.

When running AI big data tasks, due to constraints like region, equipment,
and cluster size, we often need to **select an appropriate Worker Cluster for each task**.
Among multiple clusters, **Karmada is undoubtedly the best choice**;
however, the current lack of scalability in the Scheduler prevents precise cluster selection through gang scheduling.
Especially in AI operations, such as tasks requiring extensive parallel processing and data-intensive tasks,
the current scheduling strategies have not sufficiently considered the specific hardware needs
(such as GPUs) of AI tasks,
nor the sensitivity of these tasks to immediate resource configuration.
If Karmada could enhance its capabilities in the aforementioned areas,
it would possess the ability to handle more scenarios across multiple clusters.

#### Story 2ÔºöMulti-Cluster Scheduling for Large-Scale AI Data Tasks

<!--
‰Ωú‰∏∫‰∏Ä‰∏™Áî®Êà∑ÔºåÊàëÂ∏åÊúõÂ∞Ü Karmada ‰Ωú‰∏∫‰∏Ä‰∏™Â§ßÂûãÁöÑ AI Â§ßÊï∞ÊçÆ‰ªªÂä°ÁöÑË∞ÉÂ∫¶Âπ≥Âè∞ÔºåÂÆÉÂ∫îÂΩìËÉΩÂ§üÂÅöÂà∞Â∞ÜÂ§ßÂûãÁöÑ**Â§öÊ®°Êùø‰ªªÂä°**ÊãÜÂàÜÂà∞Â§ö‰∏™ Worker Cluster‰∏≠„ÄÇ

Âú®‰ªªÂä°Ë∞ÉÂ∫¶‰∏≠ÔºåÊúâÁõ∏ÂΩìÂ§ßÊØî‰æãÁöÑ‰ªªÂä°ÊòØÂ§öÊ®°Êùø‰ªªÂä°ÔºàTensorFlowJobÔºåMPIJobÔºåPyTorchJobÁ≠âÁ≠âÔºâÔºåÂú®‰∏Ä‰∫õÊÉÖÂÜµ‰∏ãÂçï‰∏™ Worker Cluster
ËµÑÊ∫ê‰∏çË∂≥‰ª•ÂÆåÊàê‰ªªÂä°ÔºåÊàëÂ∏åÊúõÂèØ‰ª•ÈÄöËøá Karmada Â∞ÜÂ§öÊ®°Êùø‰ªªÂä°ËæÉ‰∏∫Á≤æÁ°ÆÁöÑÊãÜÂàÜÂà∞Â§ö‰∏™ÈõÜÁæ§‰∏≠Âπ∂Ë°åÂ§ÑÁêÜÔºåÂ∞§ÂÖ∂ÊòØÂú®Á¢éÁâáËæÉÂ§öÁöÑÂú∫ÊôØ‰∏ã„ÄÇ

Èô§‰∫ÜÈúÄË¶ÅÂ∞Ü‰ªªÂä°ÊãÜÂàÜÔºåÂæàÊúâÂèØËÉΩ‰ºöÊúâÊõ¥Â§çÊùÇÁöÑÁ≠ñÁï•ÈúÄÊ±ÇÔºåÂ¶Ç Master Âíå Worker Â∏åÊúõÊåâÊØî‰æãË∞ÉÂ∫¶Âà∞‰∏çÂêåÁöÑ Worker Cluster ‰∏≠ÔºåÊàñÊòØ‰ºòÂÖàË∞ÉÂ∫¶ Master Á≠â„ÄÇ
-->

As a user, I want to use Karmada as a scheduling platform for large-scale AI big data tasks,
capable of splitting **large multi-template tasks** across multiple Worker Clusters.

In task scheduling, a significant proportion of tasks are multi-template tasks
(such as TensorFlowJob, MPIJob, PyTorchJob, etc.).
In some cases, when a single Worker Cluster does not have enough resources to complete the task,
I hope to use Karmada to accurately split multi-template tasks into several clusters for parallel processing,
especially in scenarios with many fragments.

In addition to needing to split tasks, there is likely a need for more complex strategy requirements,
such as scheduling *Masters* and *Workers* proportionally to different **Worker Clusters**,
or giving priority to scheduling the *Master*.

## Design Details

### Scheduler Extensibility

#### Making AssignReplicas Logic the Default Plugin Content

<!--
Êàë‰ª¨ÂèØ‰ª•Â∞ÜÁõÆÂâç AssignReplicas ÁöÑÈÄªËæë‰Ωú‰∏∫ Default AssignReplicas Êèí‰ª∂ÁöÑÈÄªËæëÔºå‰ªéËÄåÂÖºÂÆπ Karmada Scheduler ÁõÆÂâçÊâÄÊîØÊåÅÁöÑÊâÄÊúâÁ≠ñÁï•„ÄÇ
Áî®Êà∑ÂèØ‰ª•ÈÄöËøáÂÄüÈâ¥ Default Êèí‰ª∂ÊàñÊòØÂú® Default Êèí‰ª∂ÁöÑÂü∫Á°Ä‰πã‰∏äÂºÄÂèëËá™Â∑±ÁöÑËá™ÂÆö‰πâÊèí‰ª∂Êù•ÂÆûÁé∞‰∏çÂêåÁöÑË∞ÉÂ∫¶Á≠ñÁï•„ÄÇ
-->


We can adopt the current `AssignReplicas` logic as the logic for the **Default AssignReplicas plugin**,
thereby supporting all strategies currently supported by the Karmada Scheduler.  
Users can develop their own custom plugins based on the Default plugin or by referencing it,
to implement different scheduling strategies.

#### Register Custom Scheduling Policies to Scheduler Framework

<!--
Êàë‰ª¨Âú®ÁõÆÂâç Scheduler ÁöÑÂü∫Á°Ä‰∏äËÆæËÆ°‰∫Ü‰∏ÄÁßçÊîØÊåÅ Plugin Ê≥®ÂÜåËá™ÂÆö‰πâÁ≠ñÁï•ÁöÑÊñπÂºèÔºåÂΩì Scheduler Framework Init Êó∂ÔºåÂ∞ÜÊâÄÊúâ Plugin ÊîØÊåÅÁöÑËá™ÂÆö‰πâ
Á≠ñÁï•Ê≥®ÂÜåÂà∞ Scheduler ÂÜÖ„ÄÇScheduler Cache ‰∏≠Â∫îÁºìÂ≠òÊØè‰∏™ Custom Strategy ÂØπÂ∫îÁöÑ Plugin„ÄÇ
-->

We have designed a method to support plugin registration of custom strategies based on the current Scheduler.
When the Scheduler Framework initializes,
it registers all the custom strategies supported by the plugins into the Scheduler.  
The Scheduler Cache should cache the plugin corresponding to each Custom Strategy.

##### AssignReplica Plugin Interface

<!--
Êàë‰ª¨ÂèØ‰ª•Âú®ÁõÆÂâçÁöÑ Plugin Interface Âü∫Á°Ä‰∏äÔºåÊñ∞Â¢û AssignReplicaPluginÔºåÂú® Framework ÂàùÂßãÂåñÊó∂ÂèØ‰ª•ÈÄöËøá Strategies() Êù•ÂæóÂà∞ËØ•Êèí‰ª∂ÊâÄÊîØÊåÅÁöÑ
Ëá™ÂÆö‰πâÁ≠ñÁï•ÂêçÁß∞ÔºåÂ¶ÇÊûú PropagationPolicy ÈúÄË¶Å‰ΩøÁî®ÁöÑ Custom Strategy Â±û‰∫éËØ•Êèí‰ª∂ÔºåÂàôË∞ÉÁî®ËØ•Êèí‰ª∂ÁöÑ AssignReplica ‰Ωú‰∏∫ËøîÂõû„ÄÇ
-->

We can add an `AssignReplicaPlugin` based on the current Plugin Interface.
During the initialization of the Framework,
it is possible to obtain the names of the custom strategies supported by this plugin through `Strategies()`.  
If the `Custom Strategy` required by the `PropagationPolicy` belongs to this plugin,
then the plugin's `AssignReplica` method will be invoked as the response.

```go
// Plugin is the parent type for all the scheduling framework plugins.
type Plugin interface {
    Name() string
}

// FilterPlugin is an interface for filter plugins. These filters are used to filter out clusters
// that are not fit for the resource.
type FilterPlugin interface {
    Plugin
    // Filter is called by the scheduling framework.
    Filter(ctx context.Context, bindingSpec *workv1alpha2.ResourceBindingSpec, bindingStatus *workv1alpha2.ResourceBindingStatus, cluster *clusterv1alpha1.Cluster) *Result
}

// üåü New üåü
type AssignReplicaPlugin interface {
    Plugin
    Strategies() []string
    AssignReplica(ctx context.Context, customStrategyName string, clusters []*clusterv1alpha1.Cluster, spec *workv1alpha2.ResourceBindingSpec,
        status *workv1alpha2.ResourceBindingStatus) ([]workv1alpha2.TargetCluster, error)
}
```

#### Select Custom Scheduling Policies

<!--
Êàë‰ª¨ÈúÄË¶ÅÂÖÅËÆ∏‰∏Ä‰∫õËµÑÊ∫êËá™Áî±ÁöÑÈÄâÊã©ÂÆÉÈúÄË¶ÅÁöÑË∞ÉÂ∫¶Á≠ñÁï•ÔºåËÄå‰∏çÊòØ‰ªÖ‰ªÖÂ±ÄÈôê‰∫é Karmada ÈªòËÆ§Êèê‰æõÁöÑÁ≠ñÁï•Ôºå‰ΩÜÊòØÁõÆÂâç PropagationPolicy ÁöÑ
ReplicaSchedulingStrategy Âπ∂‰∏çÊîØÊåÅËá™Áî±ËæìÂÖ•‰∏Ä‰∫õÂÜÖÂÆπÔºåÂè™ËÉΩÂÜôÂÖ•Â¶Ç Enum=Duplicated;Divided ÁöÑÂõ∫ÂÆöÂÄº„ÄÇ

Ë¶ÅÂÆûÁé∞Ëøô‰∏™ËÉΩÂäõÔºåÊàë‰ª¨ÂèØ‰ª•Âú® PropagationPolicy ‰∏≠ÂÖÅËÆ∏Áî®Êà∑ÈÄâÊã©Ëá™ÂÆö‰πâÁ≠ñÁï•ÔºåÂ¶ÇÊ≠§ Scheduler Â∞±ÂèØ‰ª•Áü•ÈÅìË¶ÅÈÄâÊã©Áî®Âì™‰∏Ä‰∏™ AssignReplica Êèí‰ª∂ÁöÑË∞ÉÂ∫¶ÁªìÊûú„ÄÇ
-->

We need to allow some resources the freedom to choose their desired scheduling strategies,
rather than being limited to the strategies by default provided by Karmada.
However,
the `ReplicaSchedulingStrategy` in the current `PropagationPolicy` does not support free input
and can only accept fixed values like `Enum=Duplicated;Divided`.

To implement this capability, we can allow users to select custom strategies in `PropagationPolicy`.
This way, the Scheduler will know which `AssignReplica` plugin's scheduling results to choose.

```go
type ReplicaSchedulingStrategy struct {
    // ReplicaSchedulingType determines how the replicas is scheduled when karmada propagating
    // a resource. Valid options are Duplicated and Divided.
    // "Duplicated" duplicates the same replicas to each candidate member cluster from resource.
    // "Divided" divides replicas into parts according to number of valid candidate member
    // clusters, and exact replicas for each cluster are determined by ReplicaDivisionPreference.
    // +kubebuilder:validation:Enum=Duplicated;Divided
    // +kubebuilder:default=Divided
    // +optional
    ReplicaSchedulingType ReplicaSchedulingType `json:"replicaSchedulingType,omitempty"`

    // ReplicaDivisionPreference determines how the replicas is divided
    // when ReplicaSchedulingType is "Divided". Valid options are Aggregated and Weighted.
    // "Aggregated" divides replicas into clusters as few as possible,
    // while respecting clusters' resource availabilities during the division.
    // "Weighted" divides replicas by weight according to WeightPreference.
    // +kubebuilder:validation:Enum=Aggregated;Weighted
    // +optional
    ReplicaDivisionPreference ReplicaDivisionPreference `json:"replicaDivisionPreference,omitempty"`

    // WeightPreference describes weight for each cluster or for each group of cluster
    // If ReplicaDivisionPreference is set to "Weighted", and WeightPreference is not set, scheduler will weight all clusters the same.
    // +optional
    WeightPreference *ClusterPreferences `json:"weightPreference,omitempty"`
	
    // üåü New üåü
    // +optional
	CustomSchedulingStrategy string `json:"customSchedulingStrategy,omitempty"`
}
```

<!--
CustomSchedulingStrategy Â≠óÊÆµÂÖÅËÆ∏Áî®Êà∑Áõ¥Êé•ÈÖçÁΩÆËá™ÂÆö‰πâÁ≠ñÁï•ÁöÑÂêçÁß∞ÔºåÊ≠§Â≠óÊÆµ‰∏∫ÂèØÈÄâÔºåÂ¶ÇÊûúËÆæÁΩÆ‰∏∫ `default` Âàô‰∏éÊú™ËÆæÁΩÆÂêå‰πâ„ÄÇ
ÂÆÉÈÄöËøá PropagationPolicy ÂëäËØâË∞ÉÂ∫¶Âô®Âú®Ë∞ÉÂ∫¶ËøáÁ®ã‰∏≠Â¶Ç‰ΩïÈÄâÊã© AssignReplica Èò∂ÊÆµÁöÑÊèí‰ª∂ÂíåËØ•Êèí‰ª∂ÂØπÂ∫îÁöÑÁ≠ñÁï•ÔºåÊàë‰ª¨ËÆ§‰∏∫Âçï‰∏™Êèí‰ª∂ÊòØÊúâÂèØËÉΩÂØπÂ∫îÂ§ö‰∏™Ëá™ÂÆö‰πâÁ≠ñÁï•ÁöÑ„ÄÇ
ÊØîÂ¶ÇÂú®Êàë‰ª¨È¢ÑÊÉ≥ÁöÑ `gang` Êèí‰ª∂‰∏≠ÔºåÂÆÉÂ∫îÂΩìÊîØÊåÅ `gang-divided` Âíå `gang-no-divided` ‰∏§‰∏™Á≠ñÁï•„ÄÇ

Âú®ËøôÈáåË¶ÅÊ≥®ÊÑèÁöÑÊòØÔºåÂΩìÁî®Êà∑ÈÖçÁΩÆ CustomReplicaSchedulingOptions ÂêéÔºåÂèØËÉΩ‰ºöÂØºËá¥Ë∞ÉÂ∫¶Âô®ÂøΩÁï•Áî®Êà∑ÈÖçÁΩÆÁöÑÂÖ∂‰ªñË∞ÉÂ∫¶ËÆæÁΩÆÔºàÂ¶ÇReplicaSchedulingTypeÔºâ„ÄÇ
ÊòØÂê¶‰ΩøÁî®Ëøô‰∫õÂèÇÊï∞Â∞Ü‰ºöÁî±Ëá™ÂÆö‰πâÊèí‰ª∂Êù•ÂÜ≥ÂÆö„ÄÇÊØîÂ¶Ç A Êèí‰ª∂ÂÆÉÂü∫‰∫é Divided Aggregated Á≠ñÁï•ÂÆûÁé∞‰∫Ü‰∏Ä‰∫õÂÖ∂‰ªñÁöÑËÉΩÂäõÔºåÈÇ£‰πàÂÆÉÂ∞±ÈúÄË¶Å‰ΩøÁî®Áî®Êà∑ÈÖçÁΩÆÁöÑÂÖ∂‰ªñÂÜÖÂÆπÔºå
ÂÜçÊØîÂ¶Ç B Êèí‰ª∂ÂÆÉÂÆûÁé∞‰∫Ü‰∏Ä‰∏™ÂÆåÂÖ®‰∏çÂêå‰πãÂâçÁöÑË∞ÉÂ∫¶Á≠ñÁï•ÔºåÈÇ£‰πàÁî®Êà∑ÈÖçÁΩÆÁöÑÂÜÖÂÆπÂæàÊúâÂèØËÉΩÊòØÊØ´Êó†ÊÑè‰πâÁöÑ„ÄÇ
-->

The `CustomSchedulingStrategy` field allows users to directly configure the name of a custom strategy.
This field is optional; if set to `default`, it is synonymous with being unset.  
It informs the scheduler via the `PropagationPolicy`
on how to select the `AssignReplica` plugin and its corresponding strategy during the scheduling process.
We believe that a single plugin could correspond to multiple custom strategies.  
For example, in our envisioned `gang` plugin, it should support both `gang-divided` and `gang-no-divided` strategies.

It is important to note that when users configure `CustomReplicaSchedulingOptions`,
it may lead the scheduler to ignore other scheduling settings configured by the users, such as `ReplicaSchedulingType`.  
Whether to use these parameters will be determined by the custom plugin.
For instance, Plugin A,
which is based on the Divided Aggregated strategy,
may need to use other user-configured content to implement additional capabilities.  
Conversely, Plugin B,
which implements a completely different scheduling strategy, might find the user-configured content irrelevant.

##### New PropagationPolicy Example

<!--
Âú®ËØ•‰æãÂ≠ê‰∏≠, Áî®Êà∑ÈÄöËøáËÆæÁΩÆ customSchedulingStrategy Êù•ÂêØÁî®Ëá™ÂÆö‰πâÁöÑ gang Ë∞ÉÂ∫¶Êèí‰ª∂,
ÈÄöËøáÂÆÉÊù•ÂÆûÁé∞Ëá™ÂÆö‰πâÁöÑ gang Ë∞ÉÂ∫¶ËÉΩÂäõ, Â¶ÇÂ§öÊ®°Êùø‰ªªÂä°ÁöÑË∞ÉÂ∫¶ÈôêÂà∂‰ª•ÂèäÁ¢éÁâáÂåñÁöÑÂÆπÈáèÂ§ÑÁêÜ, ÂΩìËµÑÊ∫êËøá‰∫éÁ¢éÁâáÂåñÂØºËá¥Êó†Ê≥ïÂºÄÂßã‰ªªÂä°Êó∂Êèí‰ª∂Â∞ÜËøîÂõûÊó†ÂèØÁî®Ë∞ÉÂ∫¶ÁªìÊûú.
-->

In this example, the user enables a custom gang scheduling plugin by setting **CustomReplicaSchedulingOptions**.
This plugin facilitates custom gang scheduling capabilities,
such as scheduling restrictions for multi-template tasks and handling fragmented capacities.
When resources are too fragmented to start a task, the plugin will return **no available scheduling results**.

```yaml
apiVersion: policy.karmada.io/v1alpha1
kind: PropagationPolicy
metadata:
  name: mindspore-gpu-task
spec:
  resourceSelectors:
    - apiVersion: batch.volcano.sh/v1alpha1
      kind: Job
      name: mindspore-gpu
  placement:
    replicaScheduling:
      customSchedulingStrategy: gang-divided
```

<!--
ÂØπ‰∫éÊ≤°ÊúâËøôÁ±ªÈúÄÊ±ÇÁöÑÁî®Êà∑Ôºå‰ªñ‰ª¨Êó†ÈúÄËÆæÁΩÆËØ•Â≠óÊÆµÔºå‰ªçÁÑ∂‰∏é‰ªéÂâçÁöÑÁî®Ê≥ï‰øùÊåÅ‰∏ÄËá¥ÔºåÊàñÊòØÂèØ‰ª•ËÆæÁΩÆ‰∏∫default„ÄÇ
-->

For users who do not have such requirements, they do not need to set this field and can continue to use it as before,
or they can set it to ‚Äúdefault.‚Äù

```yaml
apiVersion: policy.karmada.io/v1alpha1
kind: PropagationPolicy
metadata:
  name: nginx-propagation
spec:
  resourceSelectors:
    - apiVersion: apps/v1
      kind: Deployment
      name: nginx
  placement:
    replicaScheduling:
      replicaSchedulingType: Divided
      replicaDivisionPreference: Aggregated
      customSchedulingStrategy: default
```

#### Select the Result When AssignReplica

<!--
Karmada Scheduler ÁöÑ AssignReplica Â§ÑÁêÜÈò∂ÊÆµÊúâÂ§ö‰∏™Êèí‰ª∂ÊòØÂèØËÉΩÁöÑÔºåÊàë‰ª¨ÈúÄË¶ÅÊúâ‰∏ÄÁßçÊñπÊ≥ïÊù•ÈÄâÊã©Áî®Êà∑Â∏åÊúõÁöÑÁªìÊûú„ÄÇ
AssignReplicaÈò∂ÊÆµÁöÑËøîÂõûÊòØ TargetClusterÔºå‰πüÂ∞±ÊòØË∞ÉÂ∫¶ÁªìÊûúÔºåÂêåÊó∂Âú®Ëøô‰∏™Èò∂ÊÆµ‰∏≠‰ºöÊ†πÊçÆÁî®Êà∑ÈÖçÁΩÆÁöÑ ReplicaScheduling Êù•ÈÄâÊã©Á≠ñÁï•„ÄÇ
Âú®ÊúâÂ§ö‰∏™ AssignReplica Êèí‰ª∂ÁöÑÊÉÖÂÜµ‰∏ãÔºåÊàë‰ª¨Êó†Ê≥ïÁÆÄÂçïÁöÑÂÉè FilterCluster Âíå ScoreCluster ‰∏ÄÊ†∑Â∞ÜÁªìÊûúÂèñ‰∫§ÈõÜ„ÄÇ
Âõ†‰∏∫ AssignReplica ÁöÑËøîÂõûÂÄºÂÆûÈôÖ‰∏äÊòØÂêÑËá™Áã¨Á´ãÁöÑÔºåÊó†Ê≥ïÂêàÂπ∂‰∏∫‰∏Ä‰∏™ÁªìÊûúÔºåÂú®ËøôÈáåÊàë‰ª¨ÂèØ‰ª•Â∞ÜÈÄâÊã©Êèí‰ª∂ÁªìÊûúÁöÑËÉΩÂäõ‰∫§ÁªôÁî®Êà∑ÔºåÁî±Áî®Êà∑Âú®
PropagationPolicy ‰∏≠ÊåáÂÆöË¶ÅË∞ÉÁî® AssignReplica ÁöÑÂì™‰∏™Êèí‰ª∂„ÄÇ

Ê≠§ÈÉ®ÂàÜÂèØ‰ª•Âíå [Custom Scheduling Policies](#custom-scheduling-policies) ÁªìÂêàÊù•ÁêÜËß£„ÄÇ
-->

**Karmada Scheduler's `AssignReplica` Phase:**

The `AssignReplica` processing stage in the Karmada Scheduler can involve multiple plugins,
and it's necessary to have a method for selecting the desired outcome as per user preference.
The result of the `AssignReplica` phase is the `TargetCluster`, which is essentially the scheduling outcome.
Additionally, during this phase, the strategy is chosen based on the user-configured `ReplicaScheduling`.

Unlike the `FilterCluster` and `ScoreCluster` phases where results can be intersected,
having multiple `AssignReplica` plugins complicates matters
since the return values are independently distinct and cannot be merged into a single result.
In this scenario,
we empower the user to choose which `AssignReplica` plugin to invoke by specifying it in the `PropagationPolicy`.

This section can be better understood in conjunction with [Custom Scheduling Policies](#custom-scheduling-policies).

#### Maintain/Manage Plugins

<!--
Êàë‰ª¨ÂèØ‰ª•Â∞Ü Karmada Scheduler ÁöÑÊèí‰ª∂ËøõË°åÁ±ª‰ºº Kubernetes scheduler-plugins ÁöÑÊñπÂºèËøõË°åÁÆ°ÁêÜ„ÄÇ
Scheduler ÁöÑÊèí‰ª∂Â∞ÜÂÖ®ÈÉ®ÊîæÂú®ËØ• Repo ‰∏≠ÔºåKarmada ÂíåÁ§æÂå∫Áî®Êà∑ÂèØ‰ª•Â∞Ü‰ªñ‰ª¨Ëá™Â∑±ÁºñÂÜôÁöÑÊèí‰ª∂‰ª•ÂèäÊñáÊ°£ÊîæÂú®ËØ• Repo ‰∏≠„ÄÇ
Âú® Karmada ÂèëÁâàÁöÑÊó∂ÂÄôÔºåÂè™ÊûÑÂª∫ÂåÖÊã¨Á§æÂå∫Áª¥Êä§ÁöÑÈªòËÆ§Êèí‰ª∂„ÄÇÂ¶ÇÊûúÁî®Êà∑ÈúÄË¶ÅÔºå‰ªñÂèØ‰ª•Ëá™Â∑±ÁºñÂÜôÊèí‰ª∂ÂêéÊîæÂÖ•Âà∞ËØ• Repo ÊàñÊòØËá™Â∑±ÁöÑ Repo ‰∏≠Ôºå
ÁÑ∂ÂêéÈáçÊñ∞ÊûÑÂª∫ Karmada Scheduler Êù•ÂåÖÊã¨ÈúÄË¶ÅÁöÑÊèí‰ª∂„ÄÇ
-->

We can manage plugins for the **Karmada Scheduler** in a similar manner to the **Kubernetes scheduler-plugins**.  
All scheduler plugins will be stored in this repository, allowing both **Karmada** and community users to host their own plugins and documentation.  
During Karmada releases, only the default plugins maintained by the community will be built. If users wish,
they can write their own plugins, add them to this repository or their own,  
and then rebuild the Karmada Scheduler to include the necessary plugins.

### Multi-Template Partitioning

#### Support Multi-Template on ResourceInterpreterCustomization

<!--
Êàë‰ª¨ÈúÄË¶ÅÂú® ResourceInterpreterCustomization ‰∏≠ÊîØÊåÅÂ§öÊ®°ÊùøÁöÑ‰∏Ä‰∫õËÉΩÂäõÔºåËøôÊ†∑Áî®Êà∑Â∞±ÂèØ‰ª•ÈÄöËøáÈÖçÁΩÆ CRD ÁöÑÂØπÂ∫îËá™ÂÆö‰πâÊñπÊ≥ïÊù•ÂÆûÁé∞Â§öÊ®°ÊùøÊãÜÂàÜ„ÄÇ
Âú®ËøôÈáåÊàë‰ª• replicaResource Âíå replicaRevision ‰∏æ‰æãÔºå‰ªñ‰ª¨Âú® ResourceInterpreterCustomization ‰∏≠‰ª£Ë°® GetReplicas Âíå UpdateReplica„ÄÇ
ÁõÆÂâçËøô‰∏§‰∏™ÊñπÊ≥ïÂÖ•ÂèÇÂíåÂá∫ÂèÇÈÉΩÊòØÈíàÂØπÂçïÊ®°ÊùøËµÑÊ∫êÊù•ËÆæËÆ°ÁöÑÔºåÂ¶ÇÊûúÊàë‰ª¨ÈúÄË¶ÅÊîØÊåÅÂ§öÊ®°ÊùøËµÑÊ∫êÔºåÂÆÉ‰ª¨ÈúÄË¶ÅË¢´Êõ¥Êñ∞„ÄÇ
-->

We need to support multi-template capabilities in ResourceInterpreterCustomization,
allowing users to implement multi-template splitting through configuring the corresponding custom methods in the CRD.
Here, I use `replicaResource` and `replicaRevision` as examples,
which represent `GetReplicas` and `UpdateReplica` in ResourceInterpreterCustomization.
Currently, both the input and output of these methods are designed for single-template resources.
If we need to support multi-template resources, they need to be updated.

```lua
function GetReplicas(obj)
  replica = obj.spec.tasks[1].replicas
  requirement = kube.accuratePodRequirements(obj.spec.tasks[1].template)
  return replica, requirement
end

function ReviseReplica(obj, desiredReplica)
  obj.spec.tasks[1].replicas = desiredReplica
  return obj
end
```

<!--
Â¶Ç‰∏äÊâÄÁ§∫ÔºåÂèØ‰ª•ÁúãÂá∫Âú® GetReplicas Êó∂Âè™ËÉΩËøîÂõûÂçï‰∏™Ê®°ÊùøÁöÑ Replica Êï∞ÈáèÔºå‰ª•ÂèäÂÆÉÂØπÂ∫îÁöÑÊâÄÈúÄËµÑÊ∫ê requirement„ÄÇ
ÂêåÊó∂ÔºåReviseReplica ‰πüÁõ∏ÂêåÔºådesiredReplica Êàë‰ª¨Âè™ËÉΩ‰º†ÂÖ•‰∏Ä‰∏™ intÔºåÂ¶ÇÊûúÊòØÂ§öÊ®°ÊùøÁöÑËµÑÊ∫êÂàôÊó†Ê≥ïÁÅµÊ¥ªÂ§ÑÁêÜ„ÄÇ

Êàë‰ª¨Âú®Ê≠§ÁªôÂá∫ÊèêÂá∫‰∏ÄÁßçÂèØË°åÁöÑÊÄùË∑ØÔºåÂÖÅËÆ∏ Custom Interpreter Ê®°ÂùóÂèØ‰ª•ÊîØÊåÅÂ§öÊ®°ÊùøÔºåÊàë‰ª¨ÂèØ‰ª•Âü∫‰∫é‰∏äÊñáÊèêÂà∞ÁöÑ‰∏§‰∏™ÂçïÊ®°Êùø FuncÔºåÂÆûÁé∞Á±ª‰ººÁöÑÂ§öÊ®°ÊùøÁâàÊú¨„ÄÇ
Áü≠ÊúüÂÜÖÊàë‰ª¨Êó†Ê≥ïÂ∞ÜÂéüÊúâÁöÑ GetReplicas Âíå ReviseReplica ÂºÉÁî®ÔºåËøôÂ∞ÜÂØºËá¥Â§ßÈáèÁî®Êà∑Âú®Êõ¥Êñ∞Êó∂Âá∫Áé∞Á†¥ÂùèÊÄßÂÖºÂÆπÈóÆÈ¢ò„ÄÇ

Êñ∞ÊñπÊ≥ïÁöÑÊÄùË∑ØËæÉ‰∏∫Ê∏ÖÊô∞ÔºåGetReplicas ÁöÑËøîÂõûÂÄº‰øÆÊîπ‰∏∫ÁªìÊûÑ‰ΩìÊï∞ÁªÑÔºåÂÆÉÈúÄË¶ÅËøîÂõûÊØè‰∏™Ê®°ÊùøÁöÑ TemplateName, Replica, Requirement„ÄÇReviseReplica
ÂàôÊòØÂ∞ÜÂÖ•ÂèÇ‰øÆÊîπ‰∏∫ÁªìÊûÑ‰ΩìÊï∞ÁªÑÔºåÊàë‰ª¨ÈúÄË¶Å‰º†ÂÖ•ÊØè‰∏™ Template ÂØπÂ∫îÁöÑ Replica Êï∞ÈáèÔºå‰ªéËÄå‰øÆÊîπËµÑÊ∫ê‰∏≠ÊâÄÊúâÊ®°ÊùøÁöÑ Replica„ÄÇ

-->

```lua
function MultiGetReplicas(obj)
  local results = {}
  for i, task in ipairs(obj.spec.tasks) do
    local name = task.name
    local replica = task.replicas
    local requirement = kube.accuratePodRequirements(task.template)
    table.insert(results, {name = name, replica = replica, requirement = requirement})
  end
  return results
end

function MultiReviseReplica(obj, desiredReplicas)
  for _, update in ipairs(updates) do
    for i, task in ipairs(obj.spec.tasks) do
      if task.name == update.templateName then
        task.replicas = update.replica
      end
    end
  end
  return obj
end
```

##### Lua Example

###### MultiGetReplicas

```lua
-- Fake CRD
local crdObject = {
  spec = {
    tasks = {
      { name = "task1", replicas = 3, template = "template1" },
      { name = "task2", replicas = 2, template = "template2" }
    }
  }
}

local replicasInfo = MultiGetReplicas(crdObject)

-- Returned infos
for _, info in ipairs(replicasInfo) do
  print("Task Name:", info.name)
  print("Replicas:", info.replica)
  print("Requirements:", info.requirement)
end
```

###### MultiReviseReplica

```lua
-- Fake Updates param
local updates = {
  { templateName = "task1", replica = 5 },
  { templateName = "task2", replica = 4 }
}

local updatedObject = MultiReviseReplica(crdObject, updates)
```

##### ‰ºòÂÖàÁ∫ßÂå∫ÂàÜ

<!--
ÂΩì‰∏äËø∞ÂÜÖÂÆπË¢´ÂÆûÁé∞ÂêéÔºåCustom Interpreter ‰∏≠Â∞Ü‰ºöÂêåÊó∂Êèê‰æõ GetReplicas Âíå MultiGetReplicas ÊñπÊ≥ïÔºåÊàë‰ª¨ÂèØ‰ª•‰ºòÂÖàÂèñ MultiGetReplicas ÁöÑÁªìÊûúÔºå
ÂΩìÁî®Êà∑Êú™ÈÖçÁΩÆ MultiGetReplicas Êó∂Âàô‰ΩøÁî® GetReplicas ÊâÄÊèê‰æõÁöÑÁªìÊûú„ÄÇ
-->

##### ÂÖºÂÆπÊÄß‰ª•ÂèäÁî®Êà∑ËøÅÁßª

<!--
ÈÄöËøá‰∏äËø∞ÊñπÂºèÊàë‰ª¨ÂèØ‰ª•‰øùËØÅÊñ∞ÁâàÊú¨ÁöÑÂÖºÂÆπÊÄßÔºåÁé∞ÊúâÁî®Êà∑Â∞Ü‰∏ç‰ºöÂèóÂà∞ÂΩ±ÂìçÔºå‰ΩÜÊòØÊàë‰ª¨‰ªçÁÑ∂ÈúÄË¶ÅÈÄöËøáÁâàÊú¨Âë®ÊúüÁöÑÊñπÂºèÈÄêÊ≠•ËÆ©Áî®Êà∑‰ªé GetReplicas ËøÅÁßªÂà∞
MultiGetReplicas„ÄÇÁî®Êà∑ÁöÑËøÅÁßªÂÆûÈôÖ‰∏äÊòØÊØîËæÉÁÆÄÂçïÁöÑÔºå‰ª•‰∏ãÊòØËøÅÁßªÁöÑ‰øÆÊîπ‰æãÂ≠ê„ÄÇÈÄöËøá Webhook ÊñπÂºèÁöÑ Custom Interpreter ÂêåÁêÜ„ÄÇ

Ê≥®ÊÑèÔºå‰ª•‰∏ãÁöÑ‰æãÂ≠êÊòØÂçïÊ®°ÊùøËµÑÊ∫ê(Deployment)ÁöÑËøÅÁßªÔºåÂ§öÊ®°ÊùøËµÑÊ∫êÊ≤°ÊúâËøÅÁßªÈúÄÊ±Ç„ÄÇ
-->

###### GetReplicas -> MultiGetReplicas

```lua
function GetReplicas(obj)
  replica = obj.spec.replicas
  requirement = kube.accuratePodRequirements(obj.spec.template)
  return replica, requirement
end

function MultiGetReplicas(obj)
  local results = {}
  name = obj.spec.template.name
  replica = obj.spec.replicas
  requirement = kube.accuratePodRequirements(obj.spec.template)
  
  table.insert(results, {name = name, replica = replica, requirement = requirement})
  
  return results
end
```

##### ReviseReplica -> MultiReviseReplica

#### Update ResourceBinding API Definition

<!--
Âú®‰ªé Lua Â±ÇÈù¢ÊîØÊåÅÂ§öÊ®°ÊùøÂêéÔºåÊàë‰ª¨‰ªçÁÑ∂ÈúÄË¶Å‰øÆÊîπ ResourceBinding ÁöÑ API Êù•ÊîØÊåÅÔºåÂú® ResourceBinding ÁöÑ API ‰∏≠ÔºåËÆæËÆ°‰πãÂàùÂè™ËÄÉËôëÂà∞‰∫ÜÂçïÊ®°ÊùøËµÑÊ∫êÈúÄË¶ÅÁöÑÂÜÖÂÆπÔºå
ÊâÄ‰ª•Ëøô‰∫õÊîπÂä®Âπ∂‰∏çËΩªÊùæÔºå‰ΩÜÂ¶ÇÊûúËÉΩÂ§üÂÆûÁé∞Ëøô‰∫õËÉΩÂäõÂ∞Ü‰ºöÊèêÈ´òKarmadaÁöÑÈÄÇÁî®ËåÉÂõ¥„ÄÇ

Âú®ËøôÈáåÊàëËÆ§‰∏∫ÊúâËá≥Â∞ë 3 ‰∏™ API ÈúÄË¶ÅË¢´Êõ¥Êñ∞Ôºå‰ªñ‰ª¨Áü≠Êó∂Èó¥ÂÜÖ‰∏ç‰ºöË¢´Âà†Èô§ËÄåÊòØÈúÄË¶ÅË¢´Ê†á‰∏∫Â∫üÂºÉ‰ªéËÄåÈÅøÂÖçÁ†¥ÂùèÊÄßÊõ¥Êñ∞„ÄÇ
-->

After supporting multiple templates at the Lua level,
we still need to modify the ResourceBinding API to accommodate these changes.
Initially, the design of the ResourceBinding API only considered the needs of single-template resources,
making these modifications not straightforward.
However, implementing these capabilities would **expand the applicability of Karmada**.

Here, I believe that at least three APIs need to be updated.
**They will not be removed in the short term but should be marked as deprecated to avoid breaking changes.**

##### ResourceBindingSpec.ReplicaRequirements

<!--
Âú®ÁõÆÂâçÁöÑ ResourceBindingSpec ‰∏≠ÔºåReplicaRequirements Áî®Êù•Ë°®Á§∫Âçï‰∏™ Replica ÈúÄË¶ÅÁöÑËµÑÊ∫êÔºåÂØπ‰∫éÂ§öÊ®°ÊùøËµÑÊ∫êËøôÊ†∑ÊòØ‰∏çÈÄÇÁî®ÁöÑ„ÄÇ

Replicas ‰ª£Ë°®ÁõÆÊ†áËµÑÊ∫êÈúÄË¶ÅÁöÑ Replica Êï∞ÈáèÔºåËØ•Â≠óÊÆµÂú®Â§öÊ®°ÊùøÁöÑÊÉÖÂÜµ‰∏ã‰∏çÂ§üÂáÜÁ°ÆÔºå
Â§öÊ®°ÊùøÊÉÖÂÜµ‰∏ãÊàë‰ª¨ÂèØ‰ª•Â∞ÜÊâÄÊúâÊ®°ÊùøÈúÄË¶ÅÁöÑ Replica Á¥ØÂä†Ôºå‰ΩÜÊòØËøôÂ∞ÜÂú® AssignReplica Èò∂ÊÆµÊó∂Â∏¶Êù•È∫ªÁÉ¶Ôºå
Áî±‰∫éÊØè‰∏™Ê®°ÊùøÁöÑ Requirement ‰∏çÂêåÔºåScheduler Â∞ÜÊó†Ê≥ïÁ≤æÁ°ÆÁöÑÊ†πÊçÆÊØè‰∏™Ê®°ÊùøÈúÄË¶ÅÁöÑ Replica Êï∞ÈáèÊù•ÊãÜÂàÜÁõÆÊ†áËµÑÊ∫ê„ÄÇ

ÈíàÂØπ‰ª•‰∏ä‰∏§ÁÇπÔºåÊàë‰ª¨ÂèØ‰ª•Â∞ÜËøô‰∏§‰∏™Â≠óÊÆµÂêàÂπ∂‰∏∫‰∏Ä‰∏™Â≠óÊÆµÊù•Ë°®Á§∫Â§öÊ®°ÊùøÊÉÖÂÜµ‰∏ãÊØè‰∏™ Template ÊâÄÈúÄË¶ÅÁöÑ Requirement Âíå Replica„ÄÇ
-->

In the current `ResourceBindingSpec`, `ReplicaRequirements`
is used to represent the resources needed for a single replica.
This is not suitable for multi-template resources.

`Replicas` represents the number of replicas needed for the target resource.
This field is not accurate in multi-template scenarios. In cases of multiple templates,
we can sum up the replicas required by all templates, but this approach creates complications
during the `AssignReplica` stage. Since each template has different requirements,
the Scheduler will not be able to accurately distribute the target resources based on
the number of replicas required by each template.

To address these issues,
we can combine these two fields into a one
to represent the requirements and the number of replicas needed for each template in multi-template situations.

```go
type ResourceBindingSpec struct {
    // Replicas represents the replica number of the referencing resource.
    // +optional
    Replicas int32 `json:"replicas,omitempty"`
	
    // ReplicaRequirements represents the requirements required by each replica.
    // +optional
    ReplicaRequirements *ReplicaRequirements `json:"replicaRequirements,omitempty"`
	
    // üåü New üåü
    // TemplatesRequirements represents the all templates required replica and requirements.
    TemplatesRequirements []TemplateRequirement `json:"templateRequirement,omitempty"`
}

// üåü New üåü
type TemplateRequirement struct {
    // Template Name, eg. master/worker
    Name string `json:"name,omitempty"`

    // Template required replica count.
    Replicas int32 `json:"replicas,omitempty"`

    // Template requirements.
    Requirements *ReplicaRequirements `json:"requirements,omitempty"`
}
```

<!--
ÂêåÊó∂ÔºåÂéüÊú¨ÁöÑ Replicas Êó†ÈúÄÂºÉÁî®/‰øÆÊîπÔºå‰ªÖÈúÄÁÆÄÂçïÂà§Êñ≠Âç≥ÂèØÂÖºÂÆπ„ÄÇ
-->

Additionally, the original ‚ÄòReplicas‚Äô field does not need to be deprecated or modified;
we can simply make a conditional check during usage to ensure compatibility.

##### ResourceBindingSpec.Clusters

<!--
Âú®ÁõÆÂâçÁöÑ ResourceBindingSpec ‰∏≠ÔºåClusters ‰ª£Ë°®Ë¥üËΩΩË¢´Ë∞ÉÂ∫¶ÁöÑÁªìÊûúÔºåÂêåÊó∂‰πüÂåÖÂê´‰ªñÂ∫îËØ•Ë¢´ÊãÜÂàÜÂ§öÂ∞ë Replica Âà∞Âì™‰∏™ÈõÜÁæ§Ôºå
‰ΩÜÊòØÂú®Â§öÊ®°ÊùøÁöÑÊÉÖÂÜµ‰∏ãÁõÆÂâçËÉΩÂ§üË°®Á§∫ÁöÑÂÜÖÂÆπÊòØ‰∏çË∂≥ÁöÑÔºåÊàë‰ª¨ÈúÄË¶Å‰øÆÊîπ Clusters Â≠óÊÆµÂØπÂ∫îÁöÑÁªìÊûÑÔºåËÆ©‰ªñËÉΩÂ§üÊîØÊåÅÂ§öÊ®°Êùø‰ªªÂä°ÁöÑÊãÜÂàÜ„ÄÇ

Â∞Ü Replicas Â≠óÊÆµÊõ¥Êñ∞‰∏∫ ReplicaListÔºå‰ªéËÄåËÉΩÂ§üË°®Á§∫ÊØè‰∏™ Cluster Â∫îËØ•ÊãÜÂàÜÊüê‰∏™ Template ÁöÑ Replica Êï∞Èáè„ÄÇ
-->

In the current ResourceBindingSpec, the 'Clusters'
field represents the result of load scheduling and also includes how many replicas should be allocated to which cluster.
However, in multi-template scenarios, the content that can be currently represented is insufficient.
We need to modify the structure corresponding to the Clusters field to support the partitioning of multi-template tasks.

The 'Replicas' field should be updated to 'ReplicaList'
to represent the number of replicas of a certain template that should be allocated to each cluster.

```go
type ResourceBindingSpec struct {
    // Clusters represents target member clusters where the resource to be deployed.
    // +optional
    Clusters []TargetCluster `json:"clusters,omitempty"`
}

// TargetCluster represents the identifier of a member cluster.
type TargetCluster struct {
    // Name of target cluster.
    Name string `json:"name"`
	
    // Replicas in target cluster.
    // +optional 
    //+deprecated
    Replicas int32 `json:"replicas,omitempty"`

    // üåü New üåü
    // Replicas list in the target clusters.
    // +optional
    ReplicaList []TemplateReplica `json:"replicaList,omitempty"`
}

// üåü New üåü
type TemplateReplica struct {
    // Name of the template.
    TemplateName `json:"name",omitempty`

    // Replica num of the template.
    Replicas int32 `json:"replicas,omitempty"`
}
```

## Alternatives

<!--
Q: ËÉΩÂê¶ÈÄöËøá ResourceInterpreterCustomization ÁöÑ replicaResource Êù•ÂÆûÁé∞ËØ•ËÉΩÂäõÔºåÁõ¥Êé•Â∞Ü Job ÁöÑ Spec.MinAvailable ÂÄº‰Ωú‰∏∫ Spec.Replica ËøîÂõûÔºü

A: Êàë‰ª¨Â∑≤ÁªèËÄÉËôëËøáËøôÁßçÊñπÊ°àÔºå‰ΩÜÂèëÁé∞ÂÆÉË°å‰∏çÈÄö„ÄÇÂ¶ÇÊûúÊåâÊ≠§ÊñπÂºèÊìç‰ΩúÔºåÊàë‰ª¨Â∞±‰ºöÂ§±ÂéªÂä®ÊÄÅË∞ÉÊï¥ Pod Êï∞ÈáèÁöÑËÉΩÂäõÔºåÂõ†‰∏∫ Karmada SchedulerÂè™‰ºöËÆ§‰∏∫Ëøô‰∏™ Job ÈúÄË¶Å
Spec.MinAvailable ‰∏™ PodÔºåËøôÂπ∂ÈùûÊàë‰ª¨ÊâÄÊúüÊúõÁöÑÁªìÊûú„ÄÇÊàë‰ª¨Â∏åÊúõÂÆÉËÉΩÂ§üË°®Á§∫‰∏∫‰∏Ä‰∏™Âå∫Èó¥„ÄÇ
-->

**Q**: Is it possible to use `replicaResource` from `ResourceInterpreterCustomization` to implement the capability by
directly returning the `Spec.MinAvailable` value of a Job as `Spec.Replica`?

**A**: We have considered this approach, but it turned out to be impractical. If we proceed in this manner,
we would lose the ability to dynamically adjust the number of Pods, as the Karmada Scheduler would only recognize that
the Job requires `Spec.MinAvailable` Pods, which is not the desired outcome. We are looking for a representation that allows for a range.

<!--
Q: Âú® AssignReplicas Èò∂ÊÆµÔºågang Êèí‰ª∂ÁöÑËÆæËÆ°ÂèØËÉΩÊòØÊÄéÊ†∑ÁöÑ?

A: Êàë‰ª¨ÁõÆÂâçÁöÑËÆæÊÉ≥ÊòØÂ∞ÜÂçïÊ®°Êùø‰ªªÂä°ÂíåÂ§öÊ®°Êùø‰ªªÂä°ÁöÑÂ§ÑÁêÜÊñπÂºèÂàÜÂºÄÔºåÁî±‰∫éÂ§öÊ®°Êùø‰ªªÂä°ÁöÑÊãÜÂàÜËæÉ‰∏∫Â§çÊùÇÔºåÊàë‰ª¨ÁõÆÂâç‰∏çÂ∏åÊúõÂéªÊãÜÂàÜÂ§öÊ®°Êùø‰ªªÂä°Ôºå
Âú®Âçï‰∏™ÈõÜÁæ§ËµÑÊ∫êË∂≥Â§üÁöÑÊÉÖÂÜµ‰∏ã gang Êèí‰ª∂ÊâçËÉΩÂ§üÂÖÅËÆ∏ËØ•‰ªªÂä°‰∏ãÂèëÔºåÂê¶ÂàôÊåÇËµ∑ËØ•‰ªªÂä°ÔºåÁõ¥Âà∞ÂÆπÈáèË∂≥Â§ü„ÄÇ
ÂØπ‰∫éÂçïÊ®°Êùø‰ªªÂä°ÔºåÊàë‰ª¨‰ºöÂú® gang Êèí‰ª∂‰∏≠ÊãÜÂàÜÂÆÉÔºåÂπ∂‰∏îÊîØÊåÅ Spec.MinAvailable Âíå Spec.Replica ÂΩ¢ÂºèÁöÑÂå∫Èó¥ Replica Êï∞ÈáèË∞ÉÂ∫¶ËÉΩÂäõ„ÄÇ
-->

Q: What might the design of the gang plugin look like during the AssignReplicas stage?

A: Our current plan is to separate the handling of single-template tasks and multi-template tasks.
Due to the complexity of splitting multi-template tasks, we currently do not wish to split them.
The gang plugin will allow the task to be dispatched only if a single cluster resource is sufficient;
otherwise, the task will be suspended until there is enough capacity.
For single-template tasks,
we will split them within the gang plugin and support scheduling capabilities for interval replica quantities in the form of Spec.MinAvailable and Spec.Replica.

![gang-plugin](statics/gang-plugin.png)
